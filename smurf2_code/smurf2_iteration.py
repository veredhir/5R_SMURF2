import globimport osimport os.pathfrom __builtin__ import staticmethodfrom ast import literal_evalfrom itertools import izipimport mathimport numpy as npimport pandas as pdfrom Bio import SeqIOfrom seqBIn import *from multiprocessing import Pool, cpu_count# import itertoolsimport psutil# from functools import partialimport tracebackpd.options.mode.chained_assignment = None# import dask.dataframe as dd# I tried to use dask for time optimization. it didn't work well, code in gitfrom smurf2_headers import *from smurf2_const import qualsfrom smurf2_utills import *class Primer(object):    """    initialize and store the primers for each amplified region    Require initialization using primer.csv file        The header contain the amplified regions        Each row in the table contain single primer used to amplify the corresponding region.    """    def __init__(self):        self.all = {}        self.histogram = {}        self.min_size = 100        self.similarity_score_th = 0.7    def init(self, primers_path):        """        Init the primers using the primers.csv file        primers_path: string                      path to primers.csv file:                      The header contain the amplified regions                      Each row in the table contain single primer used to amplify the corresponding region.        """        primers_df = pd.read_csv(primers_path, index_col=False)        for region in primers_df.columns:            self.add_primer_for_region(int(region), primers_df[region].fillna('').tolist())        logging.info(            "Primers were update with {} regions, min primer size = {}".format(len(primers_df.columns), self.min_size))    def add_primer_for_region(self, region, primers_list):        """        Add list of primers for region        Initiate the primers histogram        region: int                The considered region        primers_list: list of strings                The list of string corresponding to the region        """        primers_list = filter(lambda x: x != '', primers_list)        self.all.update({region: primers_list})        for primer in primers_list:            self.histogram.update({primer: 0})            if len(primer) < self.min_size:                self.min_size = len(primer)    def print_histogram(self):        logging.debug("=============================")        logging.debug("PRIMERS HISTOGRAM:")        for primer, counter in self.histogram.iteritems():            logging.debug("{} --> {}".format(primer, counter))        logging.debug("=============================")    def update_primers_using_histogram(self, limit=20):        """        Detect which primers are in use, and which could be neglect        The primers histogram reflect how many reads where amplified using each primer        limit: int               The minimum histogram value for one primer to consider "in use" primer               Primers with value below limit will be removed        """        self.print_histogram()        for i, primers in self.all.iteritems():            new_primers = []            for primer in primers:                if self.histogram[primer] > limit:                    new_primers.append(primer)            logging.info(                "Region {}: # of new primers = {}, # of old primers = {}".format(i, len(new_primers), len(primers)))            self.all[i] = new_primers    def get_region_for_read(self, read):        """        Find the region for a given read        Go over all the primers, and find the primer with the best match.        If the best match similarity score is lower than minimum_match_score, read is invalid.        read: string        return: int or None                The region match for the given read, in case of invalid similarity score, return None        """        match_size = self.min_size        read_primer = read[:match_size]        best_score = self.similarity_score_th        region = None        best_primer = None        for i, primers in self.all.iteritems():            similarity_scores = [similar(read_primer, primer[:match_size]) for primer in primers]            score = max(similarity_scores)            if score >= best_score:                best_score = score                best_primer = primers[np.argmax(similarity_scores)]                region = i        # update the primers histogram from the reads:        if region is not None:            self.histogram[best_primer] += 1        return regionclass Smurf2Thresholds:    """Hold the thresholds for smurf2 computations    --Split thresholds--    max_changed_bases_rate_for_split: float                                      Limit the amount of bases change in one split to this threshold, the split                                      sequence can't be too different from the original sequence    min_coverage_for_split: float                            The split candidate must have estimated coverage >= this threshold    min_minor_prob_for_split: float                              minimum probability of second most probable base at a site                              required in order to call site a variant    --merge similar reference sequences-- min_similar_bases_rate_for_merge: float If two candidate sequences share >=    this fractional identity over their bases with mapped reads, then merge the two sequences into one for the next    iteration.    --stability-- max_priors_diff_for_stability_test: float In stable state the difference between the previous    iteration priors and the current priors " should be less then this threshold    --initialization--    min_initial_frequency: float                           In each iteration, filter the reference with frequency below this threshold    """    def __init__(self,                 max_changed_bases_rate_for_split,                 min_coverage_for_split,                 min_minor_prob_for_split,                 min_similar_bases_rate_for_merge,                 max_priors_diff_for_stability_test,                 max_distance_from_database=15,                 n_regions=5):        self.max_changed_bases_rate_for_split = max_changed_bases_rate_for_split        self.min_coverage_for_split = min_coverage_for_split        self.min_minor_prob_for_split = min_minor_prob_for_split        self.min_similar_bases_rate_for_merge = min_similar_bases_rate_for_merge        self.max_priors_diff_for_stability_test = max_priors_diff_for_stability_test        self.max_distance_from_database = max_distance_from_database        self.min_initial_frequency = min(self.min_coverage_for_split, max_priors_diff_for_stability_test)        self.min_freq_of_mapped_reads_for_region = self.min_initial_frequency / float(n_regions)        logging.info("min_coverage_for_split = {}, max_priors_diff_for_stability_test = {}, min_initial_frequency = {}"                     .format(self.min_coverage_for_split,                             self.max_priors_diff_for_stability_test,                             self.min_initial_frequency))class Smurf2Paths:    """    Holds smurf2 files paths    """    def __init__(self, working_dir):        self.reference = os.path.join(working_dir, "reference_db.csv")        self.full_reference = ""        self.current_state = os.path.join(working_dir, "curr_state.csv")        self.final_results = os.path.join(working_dir, "final_results.csv")        self.mapping = os.path.join(working_dir, "mapping.csv")        self.unique_ref_to_ref = os.path.join(working_dir, "unique_ref_id_to_ref_id.csv")        self.posteriors = os.path.join(working_dir, "posteriors.csv")        self.read_quals = os.path.join(working_dir, "reads_quals.csv")        self.reads_sequences = os.path.join(working_dir, "reads_sequences.csv")        self.prob_n = {Base.A: os.path.join(working_dir, "prob_A.csv"),                       Base.C: os.path.join(working_dir, "prob_C.csv"),                       Base.G: os.path.join(working_dir, "prob_G.csv"),                       Base.T: os.path.join(working_dir, "prob_T.csv")}        self.tmp_dir = working_dir        self.prev_prob_n = {}    def update_full_reference_path(self, fasta_path, read_len):        """        Full reference is saved with the fasta files        It produce once for each read length, and could be used again        """        self.full_reference = os.path.join(fasta_path, "full_reference_db_{}.csv".format(read_len))def df_seq_to_binary(df, seq_col):    best_binary_sequence = df[seq_col].apply(sequence_to_bin)    df[Base.A] = best_binary_sequence.apply(lambda r: r[Base.A])    df[Base.C] = best_binary_sequence.apply(lambda r: r[Base.C])    df[Base.G] = best_binary_sequence.apply(lambda r: r[Base.G])    df[Base.T] = best_binary_sequence.apply(lambda r: r[Base.T])    return dfdef _convert_bacteria_to_binary_format(ref, split_ref, reference_suffix):    """    convert the bacteria DB to binary format    Update the current intra-DB priors.    :param ref:    :param split_ref:    :param reference_suffix:    :return:    """    ref_columns = CurrentStateFormat.Bases.all + [CurrentStateFormat.Reference_id,                                                  CurrentStateFormat.Priors,                                                  CurrentStateFormat.Region,                                                  CurrentStateFormat.Weight]    # FIND BEST MATCH REFERENCES    best_match = df_seq_to_binary(ref, seq_col='base')    new_refs = best_match    if (split_ref is not None) and (not split_ref.empty):        minor = split_ref[['minor', CurrentStateFormat.Reference_id]].drop_duplicates()        best_match = best_match.merge(minor, how='left')        best_match.fillna(0, inplace=True)        best_match['major'] = 1-best_match['minor']        # if split -> change the priors to prior*(1-avg_minor) else prior stay the same.        best_match[CurrentStateFormat.Priors].update(            best_match[CurrentStateFormat.Priors] * best_match['major'])        split_ref = df_seq_to_binary(split_ref, seq_col='base')        split_ref[CurrentStateFormat.Reference_id] = split_ref[CurrentStateFormat.Reference_id].apply(lambda x: x+reference_suffix)        new_refs = pd.concat([best_match[ref_columns], split_ref[ref_columns]], ignore_index=True)    return new_refsdef _update_posterior_according_to_new_DB(ref, split_ref, full_posteriors, reference_suffix):    """    update the posteriors according the new split bacteria minors    :param ref:    :param split_ref:    :param full_posteriors:    :param reference_suffix:    :return:    """    posteriors_columns = [PosteriorsFormat.Posterior,                          PosteriorsFormat.Read_id,                          PosteriorsFormat.Ref_id]    if (split_ref is not None) and (not split_ref.empty):        best_match_ref = ref[[CurrentStateFormat.Reference_id]].drop_duplicates()        split_ref = split_ref[[CurrentStateFormat.Reference_id, 'minor']].drop_duplicates()        # Update intra-DB posterior with major times the existing posterior        best_match_with_minor = best_match_ref.merge(split_ref, how='left')        best_match_with_minor.fillna(0, inplace=True)        best_match_with_minor['major'] = 1 - best_match_with_minor['minor']        best_match_posteriors = best_match_with_minor.merge(full_posteriors,                                                            on=CurrentStateFormat.Reference_id,                                                            how='right')        best_match_posteriors[PosteriorsFormat.Posterior] = \            best_match_posteriors[PosteriorsFormat.Posterior]*best_match_posteriors['major']        # Update split bacteria posterior with minor times the existing posterior        split_ref_posteriors = split_ref.merge(full_posteriors,                                               on=CurrentStateFormat.Reference_id,                                               how='right')        split_ref_posteriors[PosteriorsFormat.Posterior] = \            split_ref_posteriors[PosteriorsFormat.Posterior] * split_ref['minor']        split_ref_posteriors[CurrentStateFormat.Reference_id] = split_ref_posteriors[CurrentStateFormat.Reference_id].apply(            lambda x: x + reference_suffix)        full_posteriors = pd.concat([best_match_posteriors[posteriors_columns],                                     split_ref_posteriors[posteriors_columns]], ignore_index=True)    return full_posteriors.drop_duplicates()def _get_new_posteriors_and_priors(posteriors_df, minors, refs):    if len(minors) > 0:        minor_fraction_avg = np.mean(minors)    else:        minor_fraction_avg = 0    ref_ids = list(set([str(r.get(CurrentStateFormat.Reference_id)) for r in refs]))    ref_ids = sorted(ref_ids, key=len, reverse=False)    if len(ref_ids) > 1:        for ref in refs:            if str(ref.get(CurrentStateFormat.Reference_id)) == ref_ids[0]:                ref[CurrentStateFormat.Priors] = ref[CurrentStateFormat.Priors] * (1 - minor_fraction_avg)            else:                ref[CurrentStateFormat.Priors] = ref[CurrentStateFormat.Priors] * minor_fraction_avg    new_posteriors = []    for _, posterior in posteriors_df.iterrows():        posterior_dict = {PosteriorsFormat.Ref_id: ref_ids[0],                          PosteriorsFormat.Read_id: posterior[PosteriorsFormat.Read_id],                          PosteriorsFormat.Posterior: posterior[PosteriorsFormat.Posterior] * (                                  1 - minor_fraction_avg)}        new_posteriors += [posterior_dict]        if minor_fraction_avg and len(ref_ids) > 1:            posterior_dict = {PosteriorsFormat.Ref_id: ref_ids[1],                              PosteriorsFormat.Read_id: posterior[PosteriorsFormat.Read_id],                              PosteriorsFormat.Posterior: posterior[PosteriorsFormat.Posterior] * (                                  minor_fraction_avg)}            new_posteriors += [posterior_dict]    return new_posteriors, refsclass Smurf2Iteration(object):    """    Hold all the data needed for smurf2 single iteration    and all the methods needed to calculate single iteration output.    """    def __init__(self, working_dir,                 prev_smurf2_iteration=None,                 fastq_path=None,                 reversed_fastq_path=None,                 primers_path=None,                 fasta_path=None,                 read_len=None,                 n_regions=5,                 update_weight_using_the_reads=False,                 # thresholds:                 max_changed_bases_rate_for_split=0.0025,  # (2.5%)                 min_coverage_for_split=0.005,  # (0.5%)                 min_minor_prob_for_split=0.08,                 min_similar_bases_rate_for_merge=0.999,  # (99%)                 max_priors_diff_for_stability_test=0.005,                 allow_split=False,                 debug_mode=True):        """        :param working_dir: string                            Current iteration data directory        :param prev_smurf2_iteration: Smurf2Iteration object                                      Previous iteration data        :param fastq_path: string                           Path to the reads fastq file        :param reversed_fastq_path: string                                    Path to the reversed reads fastq file        :param primers_path: string                             Path to csv file containing table with the regions as header,                             each column contains the primers for the specific region.        :param fasta_path: string                           Path to directory containing the fasta files for each region        :param read_len: int                         Read length        :param n_regions: int                         Amount of amplified regions        :param update_weight_using_the_reads: bool                                              Update the reference db with the number                                              of the region that were amplified according to the reads.        :param max_changed_bases_rate_for_split: float                                                 Limit the amount of bases change in one split to this threshold, the split                                                 sequence can't be too different from the original sequence        :param min_coverage_for_split: float                                       The split candidate must have estimated coverage >= this threshold        :param min_minor_prob_for_split: float                                         minimum probability of second most probable base at a site                                         required in order to call site a variant        :param min_similar_bases_rate_for_merge: float                                                 If two candidate sequences share >= this fractional identity over their bases with mapped reads,                                                 then merge the two sequences into one for the next iteration.        :param max_priors_diff_for_stability_test: float                                                   In stable state the difference between the previous iteration priors and the current priors "                                                   should be less then this threshold        :param allow_split: bool        :debug_mode: bool        """        self.debug_mode = False        self.iteration_index = 0        self.read_len = 0        self.allow_split = False        self.n_regions = n_regions        self.primers = Primer()        self.paths = Smurf2Paths(working_dir)        self.th = Smurf2Thresholds(max_changed_bases_rate_for_split,                                   min_coverage_for_split,                                   min_minor_prob_for_split,                                   min_similar_bases_rate_for_merge,                                   max_priors_diff_for_stability_test)        self.ref_format = ReferenceFormat()        self.reads_full_data_format = ReadsFullDataFormat()        self.prev_priors_for_stability_test = None        self.n_reads = 0        logging.info(            "Current process memory = {}G".format(psutil.Process(os.getpid()).memory_info()[0] / float(10 ** 9)))        # First EMIRGE iteration:        if prev_smurf2_iteration is None:            self.init_pre_process(primers_path,                                  fastq_path,                                  reversed_fastq_path,                                  fasta_path,                                  update_weight_using_the_reads,                                  allow_split,                                  debug_mode)        else:            self.init(prev_smurf2_iteration)        logging.info(            "Current process memory = {}G".format(psutil.Process(os.getpid()).memory_info()[0] / float(10 ** 9)))    @time_it    def init_pre_process(self,                         primers_path,                         fastq_path,                         reversed_fastq_path,                         fasta_path,                         update_weight_using_the_reads,                         allow_split,                         debug_mode):        """        Initialize the first smurf2 iteration        1. initiate parameters        2. Initiate the primers        3. classify reads into regions        4. Prepare the references        5. Find initial mapping        6. Find initial weight (How many region in the original reference)        7. Find initial priors        """        logging.info("ITERATION {}".format(self.iteration_index))        logging.info("number of regions = {}".format(self.n_regions))        self.n_reads = 0        self.debug_mode = debug_mode        self.iteration_index = 0        self.allow_split = allow_split        if self.debug_mode:            return        self.primers.init(primers_path)        self.calc_read_length([fastq_path, reversed_fastq_path])        self.paths.update_full_reference_path(fasta_path, self.read_len)        reads_df = self.prepare_reads(fastq_path, reversed_fastq_path)        # After classifying all the reads into regions using the primers        # update the primers used        self.primers.update_primers_using_histogram()        # Process the reference        # The reference database is given as fasta files.        # SMURF2 uses csv format.        if not self.is_processed_reference_file_exists():            self.prepare_references(fasta_path)        self.save_initial_references_DB()        self.find_mapping(reads_df)        self.update_reference_db_with_weight(update_weight_using_the_reads)        self.calc_initial_priors()    @time_it    def init(self, prev_iteration):        """        Initialize class using the previous iteration.        Copy data from previous iteration.        Find the mapping for current iteration        (The references are different since they were change in the end of the previous iteration)        :param prev_iteration: type EmirgeIteration        """        self.iteration_index = prev_iteration.iteration_index + 1        logging.info("ITERATION {}".format(self.iteration_index))        self.debug_mode = prev_iteration.debug_mode        self.n_regions = prev_iteration.n_regions        self.allow_split = prev_iteration.allow_split        self.read_len = prev_iteration.read_len        self.paths.read_quals = prev_iteration.paths.read_quals        self.paths.prev_prob_n = prev_iteration.paths.prob_n        self.paths.unique_ref_to_ref = prev_iteration.paths.unique_ref_to_ref        self.th = prev_iteration.th        self.n_reads = prev_iteration.n_reads        curr_state_df = pd.read_csv(prev_iteration.paths.current_state, index_col=False)        curr_state_df = curr_state_df[[CurrentStateFormat.Reference_id,                                       CurrentStateFormat.Region,                                       CurrentStateFormat.Weight,                                       CurrentStateFormat.Priors] +                                      CurrentStateFormat.Bases.all]        curr_state_df = curr_state_df.drop_duplicates()        curr_state_df.to_csv(self.paths.current_state, index=False)        self.prev_priors_for_stability_test = curr_state_df[[CurrentStateFormat.Reference_id,                                                             CurrentStateFormat.Priors]].drop_duplicates()        # Keep only references with significant frequency        threshold_for_filtering_bacterium = 0.1 * self.th.min_initial_frequency        reference_df = curr_state_df[curr_state_df[CurrentStateFormat.Priors] > threshold_for_filtering_bacterium]        logging.info("Current state # references before filter = {}, after filter above threshold [{}] # = {}"                     .format(len(self.prev_priors_for_stability_test),                             threshold_for_filtering_bacterium,                             len(reference_df[                                     [CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates())))        reference_df = reference_df[[CurrentStateFormat.Reference_id,                                     CurrentStateFormat.Region,                                     CurrentStateFormat.Weight] +                                    Base.all].drop_duplicates()        reference_df.to_csv(self.paths.reference, index=False)        posteriors_df = pd.read_csv(prev_iteration.paths.posteriors, index_col=False)        posteriors_df.to_csv(self.paths.posteriors, index=False)        reads_df = pd.read_csv(prev_iteration.paths.reads_sequences,                               index_col=False).drop_duplicates()        self.find_mapping(reads_df)    def is_processed_reference_file_exists(self):        """        Return true if the reference in smurf2 csv format exists, otherwise return false        """        if os.path.exists(self.paths.full_reference):            return True        else:            logging.info("full reference file {} do not exists".format(self.paths.full_reference))            return False    @time_it    def process_reference_in_region(self, fasta_path, read_length, region_ix):        """        Go over the fasta file contains all the reference sequences in one region.        Extract the relevant sequence for each reference (support paired end, 2*read_length)        Return dataframe: columns: Reference id, region, A, C, G, T in binary format.        """        logging.info("Start processing fasta, path = %s", fasta_path)        data_dicts = []        for record in SeqIO.parse(fasta_path, "fasta"):            # logging.debug("Start processing fasta, path = %s", fasta_path)            sequence = record.seq.__str__()            title = record.id.__str__()            record_dict = self.ref_format.get_ref_dict(title, region_ix, sequence_to_bin(                sequence[:read_length] + sequence[-1 * read_length:]))            data_dicts.append(record_dict)        logging.debug("Created dict for region = {}".                      format(region_ix))        ref_df = pd.DataFrame(data_dicts, columns=ReferenceFormat.temp_all)        return ref_df    @staticmethod    def extract_region_from_fasta_name(fasta_file_name, n_regions):        """        Extact the region index from fasta name        Region index should be include in the fasta file name.        Return int: region index        """        for i in range(1, n_regions + 1):            if str(i) in fasta_file_name:                return i        raise Exception("Invalid file name: {}, doesn't contain region id".format(n_regions))    @time_it    def get_mapped_references(self):        """        Read the mapping and reference dataframes and extract only the mapped references.        """        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        full_ref_df = pd.read_csv(self.paths.reference, index_col=False)        # Merging - taking only mapped references in mapped regions        # e.g after conversation with Noam and Gary at 14-03-2017, do not assume all regions were sampled correctly.        # TODO: Review it: I drop all reference in region without mapping.        mapped_reference = mapping_df[[MappingForamt.Ref_id, MappingForamt.Region]].drop_duplicates()        curr_ref_df = mapped_reference.merge(full_ref_df,                                             on=[HeadersFormat.Unique_Ref_id, HeadersFormat.Region],                                             how='left')        logging.info("Mapped references: {}/{}".format(            len(curr_ref_df.drop_duplicates(CurrentStateFormat.Reference_id)),            len(full_ref_df.drop_duplicates(CurrentStateFormat.Reference_id))))        return curr_ref_df    @time_it    def calc_initial_priors(self):        """        calc prior for each reference, e.g: P(Si) for each reference Si.        P(Si) = (sum_j(P(rj|Si)/Wi))/ sum_k(sum_j(P(rj|Sk)/Wk)            where P(rj|Si) is equal to 1 if the read j mapped to Si and zero otherwise.            if, for example, rj is mapped with the same probability to Si and Sj then, P(ri|Si) = P(ri|Sk) = 0.5.        1. calculate P(rj|Si)        2. sum_j(P(rj|Si))        3. sum_j(P(rj|Si)/Wi)        4. sum_k(sum_j(P(rj|Sk)/Wk)        5. (sum_j(P(rj|Si)/Wi))/ sum_k(sum_j(P(rj|Sk)/Wk)        """        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        # get all the mapped reference and all their data:        mapped_references = self.get_mapped_references()        # 1. calculate P(rj|Si): Calculate how many reads were mapped to single reference        # For each reads group - reference map: calculate how many reads were map to single reference.        # Since one read can be mapped to multiple reference: [amount of reads in the group]/[amount of reference this group map to]        mapping_df['mapped_reads_counter'] = mapping_df[MappingForamt.Map_weight]        # 2. sum_j(P(rj|Si))        # Sum how many weighted reads mapped to each reference (sum over different reads in different regions)        ref_with_counter_df = mapping_df.groupby(MappingForamt.Ref_id)['mapped_reads_counter'].sum().reset_index()        # Add the reference weight to each reference        curr_state_with_priors = ref_with_counter_df.merge(mapped_references, on=CurrentStateFormat.Reference_id,                                                           how="left")        # 3. sum_j(P(rj|Si)/Wi)        curr_state_with_priors[CurrentStateFormat.Priors] = curr_state_with_priors['mapped_reads_counter'].astype(            float) / curr_state_with_priors[CurrentStateFormat.Weight].astype(float)        # 4. sum_k(sum_j(P(rj|Sk)/Wk)        # sum over the priors (on the reference without the regions duplication.)        priors_weight = \            curr_state_with_priors[[CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()[                CurrentStateFormat.Priors].sum()        if len(curr_state_with_priors[CurrentStateFormat.Reference_id].unique()) != len(                curr_state_with_priors[[CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()):            logging.error("There is a bug in here!!!!! prior should be unique per reference")        # 5. (sum_j(P(rj|Si)/Wi))/ sum_k(sum_j(P(rj|Sk)/Wk): normalize by the priors overall weight.        curr_state_with_priors[CurrentStateFormat.Priors] = curr_state_with_priors[                                                                CurrentStateFormat.Priors] / priors_weight        curr_state_columns = [CurrentStateFormat.Reference_id, CurrentStateFormat.Region, CurrentStateFormat.Weight,                              CurrentStateFormat.Priors] + CurrentStateFormat.Bases.all        curr_state_with_priors = curr_state_with_priors[curr_state_columns]        curr_state_with_priors = curr_state_with_priors.drop_duplicates()        curr_state_with_priors.to_csv(self.paths.current_state)    @time_it    def update_reference_db_with_weight(self, weight_as_detected_regions):        """        For each bacterium in the DB, find the amount of amplified regions.        option 1: Using the original DB.        option 2: Using the amount of regions detected in the mapping phase.        :param reads_df:        :return:        """        full_ref_df = pd.read_csv(self.paths.reference, index_col=False)        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        all_mapped_refs = mapping_df[[MappingForamt.Ref_id, MappingForamt.Region]].drop_duplicates()        if weight_as_detected_regions:            ref_with_weight = all_mapped_refs.groupby([MappingForamt.Ref_id]).count().reset_index()        else:            ref_with_weight = full_ref_df.groupby([MappingForamt.Ref_id]).count().reset_index()        ref_with_weight.rename(columns={MappingForamt.Region: CurrentStateFormat.Weight}, inplace=True)        ref_with_weight = ref_with_weight[[CurrentStateFormat.Weight,                                           CurrentStateFormat.Reference_id]].drop_duplicates()        refs_without_weight = full_ref_df[ReferenceFormat.Bases.all + [ReferenceFormat.Ref_Id,                                                                       ReferenceFormat.Region]]        # get the actual weight for each mapped reference        ref_with_weight = pd.merge(refs_without_weight,                                   ref_with_weight,                                   on=ReferenceFormat.Ref_Id)        ref_with_weight.to_csv(self.paths.reference, index=False)    @time_it    def find_mapping(self, unique_reads_df, full_ref_df=None):        logging.info(            "Current process memory = {}G".format(psutil.Process(os.getpid()).memory_info()[0] / float(10 ** 9)))        if full_ref_df is None:            full_ref_df = pd.read_csv(self.paths.reference, index_col=False)        ref_df_copy = full_ref_df.copy()        for base in Base.all:            full_ref_df[base].update(full_ref_df[base].apply(lambda r: int(r)))            unique_reads_df[base] = unique_reads_df[base].apply(lambda r: int(r))        unique_reads_for_region_groups = unique_reads_df.groupby(HeadersFormat.Region)        rename_base_dict = {}        for base in Base.all:            rename_base_dict.update({base + '_y': base})  # keep only the reads bases            rename_base_dict.update({base + '_y': base})  # keep only the reads bases        mapping_dfs = []        for region, reads_df in unique_reads_for_region_groups:            logging.info(                "Current process memory = {}G".format(psutil.Process(os.getpid()).memory_info()[0] / float(10 ** 9)))            reads_df.reset_index(inplace=True)            ref_df = full_ref_df[full_ref_df[ReferenceFormat.Region] == region]            if ref_df.empty:                logging.warn("Skip region {}, no reference match to the region.".format(region))                continue            # For time optimization: use unique sequences, and later merge back the additional reference data.            ref_df['ref_group_idx'] = ref_df.groupby(Base.all).ngroup()            ref_df_unique = ref_df.drop_duplicates('ref_group_idx')            logging.info(                "# References in region = {}, Unique k-mer in region:= {}".format(len(ref_df), len(ref_df_unique)))            # fast mapping for reads inside the DB:            read_similar_to_ref = reads_df.merge(ref_df_unique, how='inner', on=Base.all + [HeadersFormat.Region])            read_similar_to_ref = read_similar_to_ref[                ['ref_group_idx', MappingForamt.Group_id, MappingForamt.Region, MappingForamt.Count]]            similar_reads_ids = read_similar_to_ref[MappingForamt.Group_id].unique()            logging.info("Similar reads groups ={} [# reads = {}], overall reads groups = {}"                         .format(len(similar_reads_ids),                                 read_similar_to_ref[MappingForamt.Count].sum(),                                 len(reads_df)))            # Map exterior reads:            reads_df = reads_df[~reads_df[MappingForamt.Group_id].isin(similar_reads_ids)]            row_size_ref_df = get_row_size(ref_df_unique)            row_size_reads_df = get_row_size(reads_df)            merged_row_size = row_size_reads_df + row_size_ref_df            merge_size = merged_row_size * len(reads_df) * len(ref_df_unique) / float(10 ** 9)            free_memory = psutil.virtual_memory().free / float(10 ** 9)            n_cpu = cpu_count()            n_chunk = int(5 * int(merge_size / float(free_memory) + 1))            logging.info(                'Mapping for region = {}, reads size = {}, ref size = {},  amount of chunks = {}, cores={}, '                'free memory = {}G'.format(region, len(reads_df), len(ref_df_unique), n_chunk, n_cpu, free_memory))            # Find the number of matches between each read - ref couple:            min_score_for_mapping = 2 * self.read_len - self.th.max_distance_from_database            reads_and_refs_df = parallelize_mapping(reads_df, ref_df_unique, 2 * self.read_len, n_chunk,                                                    min_score_for_mapping)            logging.debug("Done parallelize mapping for region")            # add intra DB reads:            reads_and_refs_df = pd.concat([read_similar_to_ref, reads_and_refs_df], ignore_index=True)            # take the unique_reference_id from the ref_id dataframe            reads_and_refs_df = reads_and_refs_df.merge(ref_df[[ReferenceFormat.Ref_Id, 'ref_group_idx']],                                                        on='ref_group_idx',                                                        how='left')            reads_and_refs_df = reads_and_refs_df[[MappingForamt.Ref_id,                                                   MappingForamt.Group_id,                                                   MappingForamt.Count,                                                   MappingForamt.Region]]            mapping_dfs.append(reads_and_refs_df)            logging.info("Found mapping for region = {}".format(region))        mapping_df = pd.concat(mapping_dfs)        unique_mapped_reads_groups = mapping_df[[MappingForamt.Group_id]].drop_duplicates()        unique_reads_df = unique_reads_df.merge(unique_mapped_reads_groups, how='right').drop_duplicates()        unique_reads_df.to_csv(self.paths.reads_sequences, index=False)        min_mapped_reads_for_kmer = int(self.n_reads * self.th.min_freq_of_mapped_reads_for_region)        logging.info(            'min_mapped_reads_for_k-mer = {}, mapping size before filter = {}'.format(min_mapped_reads_for_kmer,                                                                                      len(mapping_df)))        logging.info(            'Mapped reads before filter = {}'.format(                mapping_df.drop_duplicates(MappingForamt.Group_id)[MappingForamt.Count].sum()))        mapping_df['# mapped reads'] = mapping_df.groupby([MappingForamt.Ref_id, MappingForamt.Region])[            MappingForamt.Count].transform('sum')        logging.info("Before filter low frequency regions: # Bacteria = {}, # Bacteria k-mer = {}".                     format(len(mapping_df[MappingForamt.Ref_id].unique()),                            len(mapping_df[[MappingForamt.Ref_id, MappingForamt.Region]].drop_duplicates())))        mapping_df = mapping_df[mapping_df['# mapped reads'] > min_mapped_reads_for_kmer]        logging.info(            'min_mapped_reads_for_region = {}, mapping size after filter = {}'.format(min_mapped_reads_for_kmer,                                                                                      len(mapping_df)))        logging.info("After filter low frequency regions: # Bacteria = {}, # Bacteria k-mer = {}".                     format(len(mapping_df[MappingForamt.Ref_id].unique()),                            len(mapping_df[[MappingForamt.Ref_id, MappingForamt.Region]].drop_duplicates())))        logging.info(            'Mapped reads after filter = {}'.format(                mapping_df.drop_duplicates(MappingForamt.Group_id)[MappingForamt.Count].sum()))        mapping_df = self.filter_similar_mapped_reference(mapping_df.copy(), ref_df_copy)        # Find to how many refs each read group was mapped(approximate) - update after filtering        mapping_df[MappingForamt.Map_weight] = mapping_df[MappingForamt.Count] / \                                               mapping_df.groupby(HeadersFormat.Group_id)[                                                   MappingForamt.Count].transform('count')        mapping_df.to_csv(self.paths.mapping, index=False)    @time_it    def filter_similar_mapped_reference(self, mapping_df, references):        logging.info("Mapping size = {}".format(len(mapping_df)))        logging.debug(            "Current process memory = {}G".format(psutil.Process(os.getpid()).memory_info()[0] / float(10 ** 9)))        references['k-mer'] = references.groupby([ReferenceFormat.Region] + Base.all).ngroup()        unique_mapping = mapping_df[[MappingForamt.Ref_id, MappingForamt.Region]].drop_duplicates()        mapped_references = unique_mapping.merge(references[['k-mer',                                                             MappingForamt.Ref_id,                                                             MappingForamt.Region]],                                                 on=[MappingForamt.Ref_id,                                                     MappingForamt.Region],                                                 how='left')        mapped_ref_by_region = []        sequences_cols = []        for r, df in mapped_references.groupby(MappingForamt.Region):            curr_seq_col = 'k-mer_' + str(r)            sequences_cols.append(curr_seq_col)            df.rename(columns={'k-mer': curr_seq_col}, inplace=True)            mapped_ref_by_region.append(df[[curr_seq_col, MappingForamt.Ref_id]])        mapped_df = None        for df in mapped_ref_by_region:            if mapped_df is None:                mapped_df = df            else:                mapped_df = mapped_df.merge(df, on=MappingForamt.Ref_id, how='outer')        mapped_df = mapped_df.drop_duplicates(sequences_cols)        logging.info("UNIQUE REFERENCES: before = {}, after = {}".format(            len(mapping_df[MappingForamt.Ref_id].unique()),            len(mapped_df)))        # Remove subset of references        mapped_df['column_set'] = mapped_df[sequences_cols].ffill(1).bfill(1).apply(frozenset, 1)        all_set = dict(zip(mapped_df[MappingForamt.Ref_id], mapped_df['column_set']))        mapped_df['super_set_id'] = mapped_df[[MappingForamt.Ref_id, 'column_set']].apply(            lambda x: get_maximal_superset(all_set, x[MappingForamt.Ref_id], x['column_set']), axis=1)        # Update unique ids to original DB ids        deprecated = mapped_df[mapped_df[MappingForamt.Ref_id] != mapped_df['super_set_id']]        deprecated_dict = dict(zip(deprecated[MappingForamt.Ref_id], deprecated['super_set_id']))        ids_converter = pd.read_csv(self.paths.unique_ref_to_ref)        ids_converter[UnqiueRefToRefFormat.Unique_id] = ids_converter[UnqiueRefToRefFormat.Unique_id].\            apply(lambda x: deprecated_dict[x] if deprecated_dict.has_key(x) else x)        ids_converter.to_csv(self.paths.unique_ref_to_ref, index=False)        # Filter subsets:        mapped_df = mapped_df[mapped_df[MappingForamt.Ref_id] == mapped_df['super_set_id']]        logging.info("UNIQUE REFERENCES after remove subsets: before = {}, after = {}".format(            len(mapping_df[MappingForamt.Ref_id].unique()),            len(mapped_df)))        mapping_df = mapping_df.merge(mapped_df[[MappingForamt.Ref_id]], how='right')        mapping_df = mapping_df[[MappingForamt.Ref_id,                                 MappingForamt.Group_id,                                 MappingForamt.Region,                                 MappingForamt.Count]]        logging.info("Mapping size = {}".format(len(mapping_df)))        logging.info("Current mapped reads: {}".format(            mapping_df.drop_duplicates(MappingForamt.Group_id)[MappingForamt.Count].sum()))        return mapping_df    @time_it    def prepare_references(self, fasta_path):        """        Change the reference format from fasta files to csv format        fasta_path: string                    path to the directory contains fasta file for each region        """        os.chdir(fasta_path)        df = pd.DataFrame(columns=ReferenceFormat.temp_all)        df.to_csv(self.paths.full_reference, index=False, header=True)        files = glob.glob("*.fasta")        for fasta_file in files:            region_ix = self.extract_region_from_fasta_name(fasta_file, len(files))            ref_df = self.process_reference_in_region(fasta_file, self.read_len, region_ix)            ref_df.to_csv(self.paths.full_reference, index=False, header=False, mode='a')    @time_it    def save_initial_references_DB(self):        """        Find the amplified references using the primers found from the reads.        1. Save the index from the original reference id to the unique reference id        Work only on the first iteration        """        reference_df = pd.read_csv(self.paths.full_reference, index_col=False)        reference_df[ReferenceFormat.Ref_Id] = reference_df[ReferenceFormat.Original_Id]        reference_df[ReferenceFormat.header].to_csv(self.paths.reference, index=False)        unique_references_index = reference_df[            [ReferenceFormat.Ref_Id, ReferenceFormat.Original_Id]].drop_duplicates()        unique_references_index.to_csv(self.paths.unique_ref_to_ref, index=False)    def calc_read_length(self, fastq_files):        records_length = []        for fastq_path in fastq_files:            for record in SeqIO.parse(fastq_path, "fastq"):                record_len = len(record.seq.__str__())                records_length.append(record_len)        self.read_len = int(np.quantile(records_length, 0.1))        logging.info("Set read length to {}".format(self.read_len))    @time_it    def prepare_reads(self, fastq_path, reversed_fastq_path, unique_group_size_fraction=None):        logging.info("Start processing: fastq path = %s, reverse reads fastq path = %s", fastq_path,                     reversed_fastq_path)        base_comp_dict = {Base.A: Base.T,                          Base.G: Base.C,                          Base.C: Base.G,                          Base.T: Base.A,                          Base.N: Base.N}        data_dicts = []        read_index = 0        all_reads_counter = 0        short_reads = 0        for (record, reversed_record) in izip(SeqIO.parse(fastq_path, "fastq"),                                              SeqIO.parse(reversed_fastq_path, "fastq")):            read = record.seq.__str__()[:self.read_len]            reversed_read = reversed_record.seq.__str__()[:self.read_len][::-1]            if len(read + reversed_read) == 2 * self.read_len:                read_quals = record.letter_annotations['phred_quality'][:self.read_len]                reversed_read_quals = reversed_record.letter_annotations['phred_quality'][:self.read_len][::-1]            else:                short_reads += 1                all_reads_counter += 1                continue            quality = read_quals + reversed_read_quals            second_read = ''.join(map(lambda base: base_comp_dict[base], reversed_read))            bin_seq = sequence_to_bin(read + second_read)            region = self.primers.get_region_for_read(read)            if (len(read + second_read) != 2 * self.read_len) or (len(quality) != 2 * self.read_len):                raise Exception("Read length {}, {}!!!".format(len(read + second_read), len(quality)))            if region is not None:                record_dict = self.reads_full_data_format.get_dict(read_index, region, bin_seq, quality)                data_dicts.append(record_dict)                read_index += 1            all_reads_counter += 1        logging.info(            "Reads with matched region found = {} out of {} reads, short_reads={}".format(read_index, all_reads_counter,                                                                                          short_reads))        reads_df = pd.DataFrame(data_dicts, columns=self.reads_full_data_format.all)        reads_df[ReadsFullDataFormat.Quals] = reads_df[ReadsFullDataFormat.Quals].astype(str)        if self.debug_mode:            reads_df.to_csv(self.paths.read_quals, columns=self.reads_full_data_format.all, index=False)        reads_df_grouped = reads_df.groupby(            [self.reads_full_data_format.Region] + self.reads_full_data_format.Bases.all)        reads_df[ReadsFormat.Group_id] = reads_df_grouped.grouper.label_info        reads_df[MappingForamt.Count] = reads_df_grouped[ReadsFormat.Id].transform(len)        reads_full_size = len(reads_df)        if unique_group_size_fraction:            unique_group_minimum_size = reads_full_size / unique_group_size_fraction            reads_df = reads_df[reads_df[MappingForamt.Count] > unique_group_minimum_size]        reads_df[[ReadsFormat.Id, ReadsFormat.Group_id, ReadsFormat.Quals]].to_csv(self.paths.read_quals, index=False)        unique_reads_df = reads_df[[MappingForamt.Count, self.reads_full_data_format.Region,                                    self.reads_full_data_format.Group_id] + self.reads_full_data_format.Bases.all].drop_duplicates()        self.n_reads = len(reads_df)        n_reads_groups = len(unique_reads_df)        logging.info("Dropped {}/{} reads (too low frequency). # of reads groups = {}".                     format(reads_full_size - self.n_reads, reads_full_size, n_reads_groups))        return unique_reads_df    @time_it    def calc_prob_n(self):        """            Pr(N=n) -> Pr(S[i] = N) for each base in each sequence, for each N {A, C, G, T}            The result is 4 list for each sequence. ProbN_A, ProbN_C, ProbN_G, ProbN_T            If read or sequence is new this round (not seen at t-1), then            there is no Pr(S|R) from previous round,            so we 1 instead. (same probability for each Pr(S|R)           Initial iteration: all reads and seqs are new, so all calcs            of Pr(N=n) use the Pr(S|R) as weighted factor instead of            previous round's posterior.        """        logging.info(            "Current process memory = {}G".format(psutil.Process(os.getpid()).memory_info()[0] / float(10 ** 9)))        # Get data:        reads_seq_df = self.get_reads_seq_df()        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        reads_df = self.get_reads_df()        current_state_df = self.get_current_state_df()        posteriors_df = self.get_posteriors_df()        logging.info("length of dataframes: reads = {}, mapping = {}, current_state_df = {}"                     .format(len(reads_df), len(mapping_df), len(current_state_df)))        reads_data_columns, ref_data_columns, prob_success_data_columns, prob_failure_data_columns, rename_dict, prev_prob_n_columns = self.get_columns_for_calc_prob_n(            Base.all)        if self.iteration_index > 0:            prev_prob_n_full_dict = {Base.A: pd.read_csv(self.paths.prev_prob_n[Base.A], index_col=False),                                     Base.C: pd.read_csv(self.paths.prev_prob_n[Base.C], index_col=False),                                     Base.G: pd.read_csv(self.paths.prev_prob_n[Base.G], index_col=False),                                     Base.T: pd.read_csv(self.paths.prev_prob_n[Base.T], index_col=False)}        else:            prev_prob_n_full_dict = {}            for base in Base.all:                prob_n_cols = [HeadersFormat.Region, MappingForamt.Ref_id] + prev_prob_n_columns[base]                prev_prob_n_full_dict[base] = pd.DataFrame(columns=prob_n_cols)        logging.info(            "Current process memory = {}G".format(psutil.Process(os.getpid()).memory_info()[0] / float(10 ** 9)))        prob_n_full_dict = parallelize_calc_prob_n_for_single_ref(mapping_df,                                                                  reads_seq_df,                                                                  current_state_df,                                                                  reads_df,                                                                  posteriors_df,                                                                  prev_prob_n_full_dict,                                                                  self.iteration_index,                                                                  prob_success_data_columns,                                                                  prob_failure_data_columns,                                                                  prev_prob_n_columns,                                                                  reads_data_columns,                                                                  rename_dict)        logging.info("Done parallelize_calc_prob_n_for_single_ref")        for base in Base.all:            base_rename_dict = {}            for i in range(2 * self.read_len):                base_rename_dict[str(i)] = str(i) + base + "_prob_n"            prob_n_full_dict[base] = pd.concat(prob_n_full_dict[base], ignore_index=True)            prob_n_for_next_itr = prob_n_full_dict[base].rename(columns=base_rename_dict)            prob_n_for_next_itr.to_csv(self.paths.prob_n[base], index=False)        return prob_n_full_dict    @time_it    def get_reads_seq_df(self):        reads_sequences = pd.read_csv(self.paths.reads_sequences, index_col=False)        for base in Base.all:            reads_sequences[base] = reads_sequences[base].apply(lambda r: bin(int(r))[2:].zfill(2 * self.read_len))            base_cols = []            for i in range(2 * self.read_len):                base_cols.append(str(i) + base)            reads_sequences[base_cols] = reads_sequences[base].apply(lambda x: pd.Series(list(x)))            reads_sequences[base_cols] = reads_sequences[base_cols].astype(np.int8)        return reads_sequences    @time_it    def get_current_state_df(self):        bases = [Base.A, Base.C, Base.G, Base.T]        current_state_df = pd.read_csv(self.paths.current_state, index_col=False)        for base in bases:            current_state_df[base] = current_state_df[base].apply(lambda r: bin(int(r))[2:].zfill(2 * self.read_len))            current_state_df[CurrentStateFormat.ProbN + base] = 0            base_cols = []            for i in range(2 * self.read_len):                base_cols.append(str(i) + base)            current_state_df[base_cols] = current_state_df[base].apply(lambda x: pd.Series(list(x)))            current_state_df[base_cols] = current_state_df[base_cols].astype(np.int8)        return current_state_df    @time_it    def get_reads_df(self):        reads_df = pd.read_csv(self.paths.read_quals, index_col=False)        reads_df[ReadsFormat.Quals] = reads_df[ReadsFormat.Quals].apply(literal_eval)        for i in range(2 * self.read_len):            reads_df[str(i) + "_prob_success"] = reads_df[ReadsFormat.Quals].apply(                lambda r: quals.ONE_MINUS_P[int(r[i])])            reads_df[str(i) + "_prob_fail"] = reads_df[ReadsFormat.Quals].apply(lambda r: quals.P_DIV_3[int(r[i])])        return reads_df    @time_it    def get_posteriors_df(self):        posteriors_df = None        if self.iteration_index > 0:            posteriors_df = pd.read_csv(self.paths.posteriors, index_col=False)        return posteriors_df    def get_columns_for_calc_prob_n(self, bases):        reads_data_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        ref_data_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        prev_prob_n_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        prob_success_data_columns = []        prob_failure_data_columns = []        rename_dict = {}        for i in range(2 * self.read_len):            for base in bases:                reads_data_columns[base] += [str(i) + base + "_read"]                ref_data_columns[base] += [str(i) + base + "_ref"]                prev_prob_n_columns[base] += [str(i) + base + "_prob_n"]                rename_dict.update({str(i) + base + "_read": str(i), str(i) + base + "_ref": str(i),                                    str(i) + base + "_prob_n": str(i)})            prob_success_data_columns += [str(i) + "_prob_success"]            prob_failure_data_columns += [str(i) + "_prob_fail"]            rename_dict.update({str(i) + "_prob_success": str(i), str(i) + "_prob_fail": str(i)})        return reads_data_columns, ref_data_columns, prob_success_data_columns, prob_failure_data_columns, rename_dict, prev_prob_n_columns    def get_columns_for_calc_likelihoods(self, bases):        reads_data_columns = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}        prob_success_data_columns = []        prob_failure_data_columns = []        rename_dict = {}        prob_n_columns = []        for i in range(2 * self.read_len):            for base in bases:                reads_data_columns[base].append(str(i) + base)                rename_dict.update({str(i) + base: str(i)})            prob_success_data_columns.append(str(i) + "_prob_success")            prob_failure_data_columns.append(str(i) + "_prob_fail")            prob_n_columns.append(str(i))            rename_dict.update({str(i) + "_prob_success": str(i), str(i) + "_prob_fail": str(i)})        return prob_n_columns, reads_data_columns, prob_success_data_columns, prob_failure_data_columns, rename_dict    @time_it    def calc_likelihoods(self, prob_n_dict):        """        Calc the P(r|S)        this is the first part of the calculation of P(S|r)        P(r|S) = (1/W_s)*Mul_k(sum_n(P(b_k|n)P(n)))        e.g:            exp ( sum over all bases in each read of                    log(                        sum over {A, C, G, T} of P(S[x] == N)*P(r[x] == N) were S is the reference which the read mapped to.                        )                )        e^(log_a + log_b) = e^(log_a)*e^(log_b) = a*b (faster way)        :return:        """        reads_seq_df = self.get_reads_seq_df()        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        reads_df = self.get_reads_df()        prob_n_columns, reads_data_columns, prob_success_data_columns, prob_failure_data_columns, rename_dict = self.get_columns_for_calc_likelihoods(            Base.all)        # all prob N dataframe has the same row size        row_size_probN = get_row_size(prob_n_dict['A'])        row_size_mapping_df = get_row_size(mapping_df)        row_size_reads_df = get_row_size(reads_df)        row_size_reads_seq = get_row_size(reads_seq_df)        merged_row_size = row_size_reads_df + row_size_mapping_df + 4 * row_size_probN + row_size_reads_seq        merge_length = mapping_df[MappingForamt.Count].sum()        merge_size = merge_length * merged_row_size        free_memory = psutil.virtual_memory().free        n_cores = cpu_count()        process = psutil.Process(os.getpid())        process_memory = process.memory_info()[0]        max_cpu_to_use = max(1, min(n_cores, int(0.5 * free_memory / process_memory)))        logging.info("Current process memory = {}G, Free memory = {}G, maximum processes = {}".format(            process_memory / float(10 ** 9), free_memory / float(10 ** 9), max_cpu_to_use))        safety_factor = 5        nChunks = safety_factor * max(1, int(merge_size / float(free_memory - max_cpu_to_use * process_memory)))        logging.info("LIKELIHOOD: number of chunks = {}, mapping size = {}, reads size = {}, merge length = {}".                     format(nChunks, len(mapping_df), len(reads_df), merge_length))        df_couples = reads_df[[ReadsFormat.Id, ReadsFormat.Group_id]].merge(            mapping_df[[MappingForamt.Group_id, MappingForamt.Ref_id]], how='right')        df_couples_split = np.array_split(df_couples, max_cpu_to_use * nChunks)        init_data = (reads_seq_df, reads_df, prob_n_dict, reads_data_columns, prob_n_columns,                     prob_success_data_columns, prob_failure_data_columns, rename_dict)        pool = Pool(max(1, max_cpu_to_use - 1), initializer_calc_likelihoods_single_chunk, (init_data,))        likelihood_list = pool.map(_calc_likelihoods_single_chunk_worker_wrapper, df_couples_split)        logging.info("after pool")        likelihood_df = pd.concat(likelihood_list, ignore_index=True)        logging.info("after concat")        pool.close()        pool.join()        global initializer_likelihoods        initializer_likelihoods = None        # Multiply by the probability of the amplified regions        likelihood_cols = likelihood_df.columns        current_state_df = pd.read_csv(self.paths.current_state, index_col=False)        ref_with_weight = current_state_df[[CurrentStateFormat.Reference_id, CurrentStateFormat.Weight]].drop_duplicates()        likelihood_with_weight = likelihood_df.merge(ref_with_weight, on=CurrentStateFormat.Reference_id)        likelihood_with_weight[HeadersFormat.Likelihood] = likelihood_with_weight[HeadersFormat.Likelihood] / \                                                           likelihood_with_weight[CurrentStateFormat.Weight].astype(float)        likelihood_df = likelihood_with_weight[likelihood_cols]        logging.debug("return likelihood df, size = {}".format(likelihood_df.memory_usage(index=True, deep=True).sum()))        return likelihood_df    @time_it    def calc_posteriors(self, prob_n_dict):        """        P(S|r) = P(r|S)P(s) / sum_i(P(r|Si)P(Si))        :param prob_n_dict:        :return:        """        likelihood_df = self.calc_likelihoods(prob_n_dict)        priors_df = pd.read_csv(self.paths.current_state, index_col=False)[            [CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()        posteriors = likelihood_df.merge(priors_df, on=HeadersFormat.Unique_Ref_id, how='left')        logging.info(            "posteriors = {} refs".format(len(posteriors.drop_duplicates(HeadersFormat.Unique_Ref_id))))        # P(r|S)P(s)        posteriors[PosteriorsFormat.Posterior] = posteriors[PosteriorsFormat.Likelihood].astype(float) * posteriors[            CurrentStateFormat.Priors].astype(float)        # denominator_df --> sum_i(P(r|Si)P(Si))        denominator_df = pd.DataFrame({'denominator': posteriors.groupby(PosteriorsFormat.Read_id)[            PosteriorsFormat.Posterior].sum()}).reset_index()        posteriors = posteriors.merge(denominator_df, how='left')        # P(S|r) = P(r|S)P(s) / sum_i(P(r|Si)P(Si))        zero_posteriors = posteriors[posteriors['denominator'] == 0]        zero_posteriors[PosteriorsFormat.Posterior] = 0        logging.info(            "zero_posteriors = {} refs".format(len(zero_posteriors.drop_duplicates(HeadersFormat.Unique_Ref_id))))        non_zero_posteriors = posteriors[posteriors['denominator'] != 0]        non_zero_posteriors[PosteriorsFormat.Posterior] = \            non_zero_posteriors[PosteriorsFormat.Posterior] / non_zero_posteriors['denominator']        logging.info(            "non_zero_posteriors = {} refs".format(                len(non_zero_posteriors.drop_duplicates(HeadersFormat.Unique_Ref_id))))        posteriors = pd.concat([zero_posteriors, non_zero_posteriors])        logging.info(            "posteriors = {} refs".format(                len(posteriors.drop_duplicates(HeadersFormat.Unique_Ref_id))))        posteriors = posteriors.fillna(0)        posteriors = posteriors[[PosteriorsFormat.Posterior,                                 PosteriorsFormat.Read_id,                                 PosteriorsFormat.Ref_id]].drop_duplicates()        posteriors.to_csv(self.paths.posteriors, index=False)    @time_it    def calc_priors(self):        """            calc the priors P(s_i) = sum_j(P(s_i|rj)/ sum_i, j(P(s_i|r_j)            where P(s|r) is the posteriors.        """        posteriors_df = pd.read_csv(self.paths.posteriors, index_col=False)        new_priors = posteriors_df.groupby(PosteriorsFormat.Ref_id)[PosteriorsFormat.Posterior].sum().reset_index()        # normalize in the reference # regions:        curr_state_df = pd.read_csv(self.paths.current_state, index_col=False)        ref_weight = curr_state_df[[CurrentStateFormat.Reference_id, CurrentStateFormat.Weight]].drop_duplicates()        new_priors = ref_weight.merge(new_priors, on=CurrentStateFormat.Reference_id, how='right')        new_priors[CurrentStateFormat.Priors] = new_priors[PosteriorsFormat.Posterior]        posteriors_sum = new_priors.Priors.sum()        new_priors[CurrentStateFormat.Priors] = new_priors[CurrentStateFormat.Priors] / posteriors_sum        new_priors = new_priors[[CurrentStateFormat.Priors, CurrentStateFormat.Reference_id]].drop_duplicates()        # get previous priors values:        old_priors = curr_state_df[[CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()        # Take only only the prior values which where not calculated this round.        old_priors = old_priors[            (~old_priors[CurrentStateFormat.Reference_id].isin(new_priors[PosteriorsFormat.Ref_id]))]        priors_df = pd.concat([old_priors, new_priors], ignore_index=True, sort=True)        # update curr state with the new priors values.        cols = [CurrentStateFormat.Reference_id,                CurrentStateFormat.Region,                CurrentStateFormat.Weight] + CurrentStateFormat.Bases.all        curr_state_df = curr_state_df[cols]        curr_state_df = curr_state_df.merge(priors_df)        curr_state_df = curr_state_df.drop_duplicates()        curr_state_df.to_csv(self.paths.current_state, index=False)    @time_it    def update_bacteria_DB(self, prob_n_dict):        curr_state_df = pd.read_csv(self.paths.current_state, index_col=False)        full_posteriors = pd.read_csv(self.paths.posteriors, index_col=False)        # create table of the probabilities: the columns contains the the probability of each base in each index - 0A,        # 0C, 0G, 0T, 1A, etc the rows are the references and the regions        prob_n_list = []        for base in Base.all:            curr_prob_n_for_base = prob_n_dict[base]            curr_prob_n_for_base['base'] = base            for i in range(2*self.read_len):                ix_col = str(i)                rename_dict = {ix_col: 'prob_n'}                curr_prob_n = curr_prob_n_for_base[[CurrentStateFormat.Reference_id,                                                    CurrentStateFormat.Region,                                                    ix_col,                                                    'base']]                curr_prob_n = curr_prob_n.assign(base_index=i)                curr_prob_n.rename(columns=rename_dict, inplace=True)                prob_n_list.append(curr_prob_n)        prob_n_full = pd.concat(prob_n_list)        prob_n_full.dropna(inplace=True)        prob_n_full.sort_values([CurrentStateFormat.Reference_id, CurrentStateFormat.Region, 'base_index', 'prob_n'],                                ascending=False, inplace=True)        prob_n_max = prob_n_full.groupby([CurrentStateFormat.Reference_id, CurrentStateFormat.Region, 'base_index'])\            .nth([0]).reset_index()        new_references = prob_n_max.groupby([CurrentStateFormat.Reference_id, CurrentStateFormat.Region])['base']\            .apply(lambda x: ''.join(x)).reset_index()        new_references = new_references.merge(curr_state_df[[CurrentStateFormat.Reference_id,                                                             CurrentStateFormat.Priors,                                                             CurrentStateFormat.Weight]].drop_duplicates(),                                              on=HeadersFormat.Unique_Ref_id)        new_split_references = None        if self.allow_split and self.iteration_index > 0:            prob_n_second_best = prob_n_full.groupby([CurrentStateFormat.Reference_id, CurrentStateFormat.Region, 'base_index']) \                .nth([1]).reset_index()            prob_n_second_best = prob_n_second_best.merge(prob_n_max,                                                          on=[CurrentStateFormat.Reference_id,                                                              CurrentStateFormat.Region,                                                              'base_index'],                                                          suffixes=("", "_max"))            prob_n_second_best['max_second_in_kmer'] = prob_n_second_best.groupby([CurrentStateFormat.Reference_id,                                                                            CurrentStateFormat.Region])['prob_n'].transform('max')            is_candidate_row = prob_n_second_best['max_second_in_kmer'] > self.th.min_minor_prob_for_split            split_ref_ids_candidates = prob_n_second_best[is_candidate_row][CurrentStateFormat.Reference_id].unique()            prob_n_second_best = prob_n_second_best[prob_n_second_best[CurrentStateFormat.Reference_id].isin(split_ref_ids_candidates)]            prob_n_second_best = prob_n_second_best.assign(is_second_prob_n_large=False)            # Modified bases must be:            # 1. With the maximal probability among other bases in the same k-mer            # 2. Probability of the second best must be larger than the probability of the third and fourth.            prob_n_second_best['is_second_prob_n_large'] = \                (prob_n_second_best['prob_n'] >= 0.9*prob_n_second_best['max_second_in_kmer']) & \                (prob_n_second_best['prob_n_max'] + 3*prob_n_second_best['prob_n'] > 1.001)            prob_n_second_best['base'] = prob_n_second_best['base'].where(prob_n_second_best['is_second_prob_n_large'], prob_n_second_best['base_max'])            prob_n_second_best['#_changes'] = prob_n_second_best.groupby(                CurrentStateFormat.Reference_id)['is_second_prob_n_large'].transform('sum')            prob_n_second_best = prob_n_second_best[prob_n_second_best['#_changes'] > 0]            prob_n_second_best['prob_n'] = prob_n_second_best['prob_n'].where(prob_n_second_best['is_second_prob_n_large'], np.nan)            prob_n_second_best['minor'] = prob_n_second_best.groupby(CurrentStateFormat.Reference_id)['prob_n'].transform('mean')            ref_minors = prob_n_second_best[[CurrentStateFormat.Reference_id, 'minor']].drop_duplicates()            ref_minors = ref_minors.merge(curr_state_df[[CurrentStateFormat.Reference_id,                                                         CurrentStateFormat.Priors,                                                         CurrentStateFormat.Weight]].drop_duplicates(),                                          on=HeadersFormat.Unique_Ref_id)            ref_minors[CurrentStateFormat.Priors] = (ref_minors['minor'] * ref_minors[CurrentStateFormat.Priors])            prob_n_second_best = prob_n_second_best.merge(ref_minors[[CurrentStateFormat.Priors,                                                                      CurrentStateFormat.Reference_id,                                                                      CurrentStateFormat.Weight]])            prob_n_second_best = prob_n_second_best[prob_n_second_best[CurrentStateFormat.Priors] >= self.th.min_coverage_for_split]            if prob_n_second_best.empty:                logging.info("All split candidate are below the coverage threshold = {}".format(self.th.min_coverage_for_split))            else:                new_split_references = prob_n_second_best.groupby([CurrentStateFormat.Reference_id,                                                                   CurrentStateFormat.Region])['base']\                    .apply(lambda x: ''.join(x)).reset_index()                new_split_references = new_split_references.merge(ref_minors[[CurrentStateFormat.Priors,                                                                              CurrentStateFormat.Reference_id,                                                                              CurrentStateFormat.Weight,                                                                              'minor']], how='left')                logging.info("Split {} references, split expected coverage threshold = {}, minor base mean threshold = {}"                             .format(len(new_split_references.drop_duplicates(CurrentStateFormat.Reference_id)),                                     self.th.min_coverage_for_split,                                     self.th.min_minor_prob_for_split))        split_reference_suffix = self.iteration_index * math.pow(10, -1 * self.iteration_index - 2)        new_refs = _convert_bacteria_to_binary_format(new_references, new_split_references, split_reference_suffix)        new_posteriors = _update_posterior_according_to_new_DB(new_references,                                                               new_split_references,                                                               full_posteriors,                                                               split_reference_suffix)        new_refs.to_csv(self.paths.current_state, index=False)        new_posteriors.to_csv(self.paths.posteriors, index=False)        logging.info("New bacteria DB size = {}, prev bacteria DB size = {}".format(len(new_refs.drop_duplicates(ReferenceFormat.Ref_Id)), len(            curr_state_df.drop_duplicates(CurrentStateFormat.Reference_id))))        return new_refs    @time_it    def is_stable_state(self, new_reference_df, is_last_iteration):        if self.iteration_index <= 0:            return False        new_df = new_reference_df[[CurrentStateFormat.Reference_id, CurrentStateFormat.Priors]].drop_duplicates()        merged = self.prev_priors_for_stability_test.merge(new_df, on=CurrentStateFormat.Reference_id, how='outer')        merged.fillna(0, inplace=True)        merged['Prior_diff'] = merged[CurrentStateFormat.Priors + "_x"] - merged[CurrentStateFormat.Priors + "_y"]        merged['is_stable'] = merged['Prior_diff'].apply(lambda r:                                                         True if abs(                                                             r) < self.th.max_priors_diff_for_stability_test else False)        if False in merged['is_stable'].tolist() and not is_last_iteration:            logging.info("Priors are not stable")            logging.info("Diff: {}".format(merged[merged['is_stable'] == False]))            return False        else:            logging.info("Stable state!")            self.do_post_process(new_reference_df)            return True    @time_it    def do_iteration(self, is_last_iteration):        prob_n_dict = self.calc_prob_n()        self.calc_posteriors(prob_n_dict)        self.calc_priors()        new_reference_df = self.update_bacteria_DB(prob_n_dict)        is_stable = self.is_stable_state(new_reference_df, is_last_iteration)        mapping_df = pd.read_csv(self.paths.mapping, index_col=False)        self.n_reads = mapping_df.drop_duplicates(MappingForamt.Group_id)[MappingForamt.Count].sum()        logging.info("Iteration {}: # reads {}".format(self.iteration_index, self.n_reads))        return is_stable    @time_it    def do_post_process(self, new_reference_df):        """        For each sequence in the result find the best match in the DB        1. find the best match in the original bit format        2. find id's match in the dict file.        :return:        """        # Normalize prior by the amount of amplified regions:        new_reference_df['Sequence'] = new_reference_df.apply(            lambda r: bin_to_sequence(r[CurrentStateFormat.Bases.A],                                      r[CurrentStateFormat.Bases.C],                                      r[CurrentStateFormat.Bases.G],                                      r[CurrentStateFormat.Bases.T],                                      2 * self.read_len), axis=1)        results_cols = [CurrentStateFormat.Reference_id,                        CurrentStateFormat.Region,                        CurrentStateFormat.Priors,                        'Sequence']        results_df = new_reference_df[results_cols]        curr_state_df = pd.read_csv(self.paths.current_state, index_col=False)        ref_weight = curr_state_df[[CurrentStateFormat.Reference_id, CurrentStateFormat.Weight]].drop_duplicates()        results_df_with_weight = results_df.merge(ref_weight, how='left')        results_df_with_weight[CurrentStateFormat.Priors] = results_df_with_weight[CurrentStateFormat.Priors]/ \                                                            results_df_with_weight[CurrentStateFormat.Weight].astype(float)        sum_prior = results_df_with_weight.drop_duplicates(CurrentStateFormat.Reference_id)[CurrentStateFormat.Priors].sum()        results_df_with_weight[CurrentStateFormat.Priors] = results_df_with_weight[CurrentStateFormat.Priors]/sum_prior        results_df = results_df_with_weight[results_cols]        results_df['changed_reference_id'] = results_df[CurrentStateFormat.Reference_id]        results_df[CurrentStateFormat.Reference_id] = results_df[CurrentStateFormat.Reference_id].apply(            lambda i: int(i))        logging.info("try to read csv: {}".format(self.paths.unique_ref_to_ref))        if os.path.exists(self.paths.unique_ref_to_ref):            logging.info("try to read csv: {}".format(self.paths.unique_ref_to_ref))        else:            logging.info("try to read csv: {} - file not exists".format(self.paths.unique_ref_to_ref))        ids_dict_df = pd.read_csv(self.paths.unique_ref_to_ref, index_col=False)        full_result = results_df.merge(ids_dict_df, on=CurrentStateFormat.Reference_id, how='left')        full_result.fillna(value=-1, inplace=True)        full_result = full_result[[CurrentStateFormat.Reference_id,                                   ReferenceFormat.Original_Id,                                   'changed_reference_id',                                   CurrentStateFormat.Region,                                   CurrentStateFormat.Priors,                                   'Sequence']]        full_result.to_csv(self.paths.final_results, index=False)initializer_calc_prob_n = Nonedef initializer_calc_prob_n_for_single_ref(init_data):    global initializer_calc_prob_n    initializer_calc_prob_n = init_datadef calc_prob_n_for_single_ref_worker_wrapper(varying_data):    return calc_prob_n_for_single_ref(initializer_calc_prob_n, varying_data)def calc_prob_n_for_single_ref((current_state_df,                               reads_seq_df,                               reads_df,                               posteriors_df,                               prev_prob_n_full_dict,                               iteration_ix,                               prob_success_data_columns,                               prob_failure_data_columns,                               prev_prob_n_columns,                               reads_data_columns,                               rename_dict),  # initializer                               (ref_group_id, mapping_ref_df)                               ):    mapping_ref_df = mapping_ref_df.reset_index()    mapping_ref_df.drop(MappingForamt.Region, axis='columns', inplace=True)    mapping_ref_df = mapping_ref_df.merge(reads_seq_df, on=MappingForamt.Group_id, how='left')    ref_full_data = mapping_ref_df.merge(current_state_df,                                         on=[CurrentStateFormat.Reference_id, CurrentStateFormat.Region], how='left',                                         suffixes=('_read', '_ref'))    ref_full_data = ref_full_data.merge(reads_df, on=MappingForamt.Group_id, how='left')    if posteriors_df is not None:        ref_full_data = ref_full_data.merge(posteriors_df, on=[ReadsFullDataFormat.Id, CurrentStateFormat.Reference_id],                                            how='left')    prob_n_for_base = {}    prob_success = ref_full_data[prob_success_data_columns].copy()    prob_success.rename(columns=rename_dict, inplace=True)    prob_fail = ref_full_data[prob_failure_data_columns].copy()    prob_fail.rename(columns=rename_dict, inplace=True)    map_weight = ref_full_data[MappingForamt.Map_weight]    if iteration_ix == 0:        posteriors = map_weight    else:        ref_full_data[HeadersFormat.Posterior] = ref_full_data[HeadersFormat.Posterior].fillna(            ref_full_data[MappingForamt.Map_weight])        posteriors = ref_full_data[HeadersFormat.Posterior]    for base in Base.all:        ref_full_data = ref_full_data.merge(prev_prob_n_full_dict[base],                                            on=[CurrentStateFormat.Reference_id, CurrentStateFormat.Region],                                            how='left')        ref_full_data.fillna(value=1, inplace=True)        prev_prob_n = ref_full_data[prev_prob_n_columns[base]].rename(columns=rename_dict)        ref_full_data = ref_full_data.drop(columns=prev_prob_n_columns[base])        reads = ref_full_data[reads_data_columns[base]].rename(columns=rename_dict)        ref_full_data = ref_full_data.drop(columns=reads_data_columns[base])        # calculate Pr(n_jk|b_ik)Pr(n_jk) where j in bacteria, i is read and k is the base index:        prob_n_for_base.update(            {base: (reads.multiply(prob_success) + (1 - reads).multiply(prob_fail)).multiply(prev_prob_n)})    # calculate Pr(b_ik): sum_{A, C, G, T}[Pr(n_jk|b_ik)Pr(n_jk)] (for each k, i, j)    prob_read_bases = prob_n_for_base[Base.A] + prob_n_for_base[Base.C] + prob_n_for_base[Base.G] + prob_n_for_base[        Base.T]    for base in Base.all:        # for each base we calculate P(nk = base):        if prob_n_for_base[base].isna().values.any():            logging.warn("None in probN {} {} {}".format(base, ref_group_id, prob_n_for_base[base].values))        # calculate Pr(b_ik|n_jk)        prob_n_for_base[base] = prob_n_for_base[base] / prob_read_bases        prob_n_for_base[base] = prob_n_for_base[base].multiply(posteriors, axis='index')        # Calculate P(Nk)for each region of the current reference:        prob_n_for_base[base][HeadersFormat.Region] = ref_full_data[HeadersFormat.Region]        prob_n_for_base[base][HeadersFormat.Unique_Ref_id] = ref_full_data[HeadersFormat.Unique_Ref_id]        prob_n_for_base[base][HeadersFormat.Posterior] = posteriors        prob_n_for_base[base] = prob_n_for_base[base].groupby([HeadersFormat.Region, HeadersFormat.Unique_Ref_id]).sum()        # Divide each sequence (reference + region) in the sum of the posteriors in the specific sequence.        prob_n_for_base[base] = prob_n_for_base[base].divide(prob_n_for_base[base][HeadersFormat.Posterior], axis=0)        # get back the region and the reference id into the data        prob_n_for_base[base].reset_index(inplace=True)        if prob_n_for_base[base].isna().values.any():            logging.warn(                "After computation: None in probN {} {} {}".format(base, ref_group_id, prob_n_for_base[base].values))    # logging.info("ref_group_id = {}, Done".format(ref_group_id))    return prob_n_for_basedef calc_prob_n_for_single_ref_synchronized(mapping_df,                                            current_state_df,                                            reads_df,                                            posteriors_df,                                            prev_prob_n_full_dict,                                            iteration_index,                                            prob_success_data_columns,                                            prob_failure_data_columns,                                            prev_prob_n_columns,                                            reads_data_columns,                                            rename_dict):    mapping_row_size = get_row_size(mapping_df)    reads_row_size = get_row_size(reads_df)    curr_state_row_size = get_row_size(current_state_df)    posteriors_row_size = get_row_size(posteriors_df)    merged_row_size = mapping_row_size + reads_row_size + curr_state_row_size + posteriors_row_size    free_memory = psutil.virtual_memory().free    # we will have one row for each read-reference combination (mapping['Count'].sum())    safety_factor = 10    max_rows_for_batch = (free_memory / merged_row_size) / safety_factor    mapping_grouped_by_ref = mapping_df.groupby(CurrentStateFormat.Reference_id)    mapping_dfs = []    single_task = []    ref_ids = []    acc_length = 0    for ref_id, map_by_ref in mapping_grouped_by_ref:        curr_length = map_by_ref[MappingForamt.Count].sum()        if (acc_length + curr_length < max_rows_for_batch) or acc_length == 0:            acc_length = acc_length + curr_length            ref_ids.append(ref_id)            single_task.append(map_by_ref)        else:            logging.info("ref ids: {}, acc length = {}".format(ref_ids, acc_length))            mapping_dfs.append((ref_ids, pd.concat(single_task)))            acc_length = curr_length            ref_ids = [ref_id]            single_task = [map_by_ref]    mapping_dfs.append((ref_ids, pd.concat(single_task)))    list_of_prob_n_for_base_dicts = []    for mapping_single_chunk in mapping_dfs:        list_of_prob_n_for_base_dicts.append(            calc_prob_n_for_single_ref(current_state_df,                                       reads_df,                                       posteriors_df,                                       prev_prob_n_full_dict,                                       iteration_index,                                       prob_success_data_columns,                                       prob_failure_data_columns,                                       prev_prob_n_columns,                                       reads_data_columns,                                       rename_dict,                                       mapping_single_chunk))    full_prob_n_for_base = {'A': [], 'C': [], 'G': [], 'T': []}    for base in Base.all:        for prob_n_for_base in list_of_prob_n_for_base_dicts:            full_prob_n_for_base[base] += [prob_n_for_base[base]]    return full_prob_n_for_basedef parallelize_calc_prob_n_for_single_ref(mapping_df,                                           reads_seq_df,                                           current_state_df,                                           reads_df,                                           posteriors_df,                                           prev_prob_n_full_dict,                                           iteration_index,                                           prob_success_data_columns,                                           prob_failure_data_columns,                                           prev_prob_n_columns,                                           reads_data_columns,                                           rename_dict):    mapping_row_size = get_row_size(mapping_df)    reads_seq_row_size = get_row_size(reads_seq_df)    reads_row_size = get_row_size(reads_df)    curr_state_row_size = get_row_size(current_state_df)    posteriors_row_size = get_row_size(posteriors_df)    merged_row_size = mapping_row_size + reads_row_size + curr_state_row_size + posteriors_row_size + reads_seq_row_size    free_memory = psutil.virtual_memory().free    #  ---splitting mapping_df to chunks---    #  Each chunk contain several references ids, according to the expected merge size    # we will have one row for each read-reference combination (mapping['Count'].sum())    n_cpu = cpu_count()    process = psutil.Process(os.getpid())    process_memory = process.memory_info()[0]    max_cpu_to_use = max(1, min(n_cpu, int(0.5 * free_memory / process_memory)))    logging.info("Current process memory = {}G, Free memory = {}G, maximum processes = {}".format(        process_memory / float(10 ** 9), free_memory / float(10 ** 9), max_cpu_to_use))    safety_factor = 5    max_rows_for_single_core = ((free_memory - process_memory * max_cpu_to_use) / merged_row_size) / safety_factor    mapping_grouped_by_ref = mapping_df.groupby(CurrentStateFormat.Reference_id)    max_group_size = mapping_grouped_by_ref[MappingForamt.Count].sum().max()    min_rows_for_batch = max_group_size    max_cores_to_use = max(1, int(max_rows_for_single_core / min_rows_for_batch))    n_cpu_to_use = min(max_cpu_to_use, max_cores_to_use)    max_rows_for_batch = max_rows_for_single_core / n_cpu_to_use    logging.info("Max rows in batch = {}, cores = {}, max rows for single reference = {}".format(max_rows_for_batch,                                                                                                 n_cpu_to_use,                                                                                                 max_group_size))    mapping_dfs = []    single_task = []    ref_ids = []    acc_length = 0    for ref_id, map_by_ref in mapping_grouped_by_ref:        curr_length = map_by_ref[MappingForamt.Count].sum()        if (acc_length + curr_length < max_rows_for_batch) or acc_length == 0:            acc_length = acc_length + curr_length            ref_ids.append(ref_id)            single_task.append(map_by_ref)        else:            # logging.info("ref ids: {}, acc length = {}".format(ref_ids, acc_length))            mapping_dfs.append((ref_ids, pd.concat(single_task)))            acc_length = curr_length            ref_ids = [ref_id]            single_task = [map_by_ref]    mapping_dfs.append((ref_ids, pd.concat(single_task)))    #  ---Done splitting mapping_df to chunks---    initializer_data = (current_state_df,                        reads_seq_df,                        reads_df,                        posteriors_df,                        prev_prob_n_full_dict,                        iteration_index,                        prob_success_data_columns,                        prob_failure_data_columns,                        prev_prob_n_columns,                        reads_data_columns,                        rename_dict)    pool = Pool(max(1, n_cpu_to_use - 1),                initializer_calc_prob_n_for_single_ref,                (initializer_data,),                maxtasksperchild=safety_factor)    logging.info("Max rows in chunk = {}, amount of chunks = {}, pool size = {}".format(max_rows_for_batch,                                                                                        len(mapping_dfs),                                                                                        max(1, n_cpu_to_use - 1)))    full_prob_n_for_base = {'A': [], 'C': [], 'G': [], 'T': []}    try:        list_of_prob_n_for_base_dicts = pool.map(calc_prob_n_for_single_ref_worker_wrapper, mapping_dfs)        logging.info("after pool")        for base in Base.all:            for prob_n_for_base in list_of_prob_n_for_base_dicts:                full_prob_n_for_base[base] += [prob_n_for_base[base]]        logging.info("after concat mapping")        pool.close()        pool.join()        global initializer_calc_prob_n        initializer_calc_prob_n = None    except Exception as ex:        logging.info("Exception, exit(1)")        traceback.print_exc()        exit(1)    return full_prob_n_for_baseinitializer_likelihoods = Nonedef initializer_calc_likelihoods_single_chunk(init_data):    global initializer_likelihoods    initializer_likelihoods = init_datadef _calc_likelihoods_single_chunk_worker_wrapper(varying_data):    return _calc_likelihoods_single_chunk(initializer_likelihoods, varying_data)def _calc_likelihoods_single_chunk((reads_sequence_df, reads_df, prob_n_dict, reads_data_columns, prob_n_columns,                                   prob_success_data_columns, prob_failure_data_columns, rename_dict),                                   df_couples):    reads_chunk = df_couples.merge(reads_df, on=[ReadsFormat.Id, ReadsFormat.Group_id], how='left')    chunk_reads_full_data = pd.DataFrame.merge(reads_chunk, reads_sequence_df, on=MappingForamt.Group_id, how='left')    res_dict = {Base.A: [], Base.C: [], Base.G: [], Base.T: []}    for base in Base.all:        chunk_full_data_df = chunk_reads_full_data.merge(prob_n_dict[base],                                                         on=[MappingForamt.Ref_id, HeadersFormat.Region],                                                         how='left')        reads = chunk_full_data_df[reads_data_columns[base]]        reads.rename(columns=rename_dict, inplace=True)        prob_n = chunk_full_data_df[prob_n_columns]        prob_success = chunk_full_data_df[prob_success_data_columns].rename(columns=rename_dict)        prob_fail = chunk_full_data_df[prob_failure_data_columns].rename(columns=rename_dict)        reads_prob = reads.multiply(prob_success) + (1 - reads).multiply(prob_fail)        res_dict[base] = prob_n.multiply(reads_prob)    res = res_dict[Base.A] + res_dict[Base.C] + res_dict[Base.G] + res_dict[Base.T]    res = res[prob_n_columns].applymap(np.log)    likelihood = pd.DataFrame({HeadersFormat.Likelihood: res.apply(sum, axis=1)}).apply(np.exp)    likelihood[MappingForamt.Ref_id] = chunk_reads_full_data[MappingForamt.Ref_id]    likelihood[ReadsFullDataFormat.Id] = chunk_reads_full_data[ReadsFullDataFormat.Id]    return likelihooddef _map_single_chunk_parallelize((ref_df, nBits, min_score_for_mapping), reads_df):    reads_and_refs_chunk_df = pd.DataFrame.merge(ref_df, reads_df, on=HeadersFormat.Region, how='right',                                                 suffixes=('_ref', '_read'))    try:        a = np.bitwise_and(reads_and_refs_chunk_df['A_ref'], reads_and_refs_chunk_df['A_read'])        c = np.bitwise_and(reads_and_refs_chunk_df['C_ref'], reads_and_refs_chunk_df['C_read'])        g = np.bitwise_and(reads_and_refs_chunk_df['G_ref'], reads_and_refs_chunk_df['G_read'])        t = np.bitwise_and(reads_and_refs_chunk_df['T_ref'], reads_and_refs_chunk_df['T_read'])        reads_and_refs_chunk_df['Score_bit'] = np.bitwise_or(np.bitwise_or(a, c), np.bitwise_or(g, t))        reads_and_refs_chunk_df['Score'] = reads_and_refs_chunk_df['Score_bit'].apply(lambda x: bin(x).count("1"))        reads_and_refs_chunk_df = reads_and_refs_chunk_df[reads_and_refs_chunk_df['Score'] ==                                                          (reads_and_refs_chunk_df.groupby(HeadersFormat.Group_id)[                                                               'Score'].transform(max))]        low_score = reads_and_refs_chunk_df[reads_and_refs_chunk_df['Score'] <= min_score_for_mapping]        if not low_score.empty:            logging.debug("Originaly {} reads, Low score:\n{}".format(len(reads_df),                                                                     low_score[                                                                         [MappingForamt.Group_id, MappingForamt.Count,                                                                          'Score']]))            low_score['seq'] = low_score.apply(lambda r: bin_to_sequence(r['A_read'],                                                                         r['C_read'],                                                                         r['G_read'],                                                                         r['T_read'],                                                                         2 * 126), axis=1)            low_score['seq_ref'] = low_score.apply(lambda r: bin_to_sequence(r['A_ref'],                                                                             r['C_ref'],                                                                             r['G_ref'],                                                                             r['T_ref'],                                                                             2 * 126), axis=1)            logging.debug("Read: {}".format(low_score['seq'].iloc[0]))            logging.debug("Ref: {}\n\n".format(low_score['seq_ref'].iloc[0]))        reads_and_refs_chunk_df = reads_and_refs_chunk_df[reads_and_refs_chunk_df['Score'] > min_score_for_mapping]    except Exception, err:        logging.error("read ids = {}".format(reads_df[ReadsFormat.Group_id]))        logging.error("reads_df length = {}".format(len(reads_df)))        logging.error("ref_df length = {}".format(len(ref_df)))        logging.error("nBits = {}, min_score_for_mapping = {}".format(nBits, min_score_for_mapping))        traceback.print_exc()        # raise Exception("read ids = {}".format(reads_df[ReadsFormat.Group_id]))        # # logging.exception("Exception = {}, reads ids = {}".format(str(ex), reads_df[ReadsFormat.Group_id]))        reads_and_refs_chunk_df = pd.DataFrame(columns=reads_and_refs_chunk_df.columns)    return reads_and_refs_chunk_df[['ref_group_idx', MappingForamt.Group_id, MappingForamt.Region, MappingForamt.Count]]initializer_mapping = Nonedef initializer_mapping_chunk(init_data):    global initializer_mapping    initializer_mapping = init_datadef mapping_single_chunk_worker_wrapper(varying_data):    return _map_single_chunk_parallelize(initializer_mapping, varying_data)def parallelize_mapping(reads_df, ref_df, nBits, nChunks, min_score_for_mapping, n_cores=cpu_count()):    df_split = np.array_split(reads_df, n_cores * nChunks)    initializer_data = (ref_df, nBits, min_score_for_mapping)    pool = Pool(max(1, n_cores - 1), initializer_mapping_chunk, (initializer_data,))    map_list = pool.map(mapping_single_chunk_worker_wrapper, df_split)    logging.debug("after pool")    res_df = pd.concat(map_list)    logging.debug("after concat")    pool.close()    pool.join()    global initializer_mapping    initializer_mapping = None    return res_dfdef get_emirge_iteration_mock():    data_path = "/home/vered/EMIRGE/EMIRGE-data/mock_data/"    reads_fastq_path = data_path + "reads_mock1.fastq"    reversed_reads_fastq_path = data_path + "reads_mock2.fastq"    working_dir = "/home/vered/EMIRGE/EMIRGE-data/mock_tests"    # reference_path = working_dir + "/full_reference_db.csv"    fasta_path = data_path    read_len = 126    emirge_iteration = Smurf2Iteration(working_dir, reads_fastq_path, reversed_reads_fastq_path,                                       fasta_path, read_len)    return emirge_iterationdef offline_post_process(last_results_path, final_results_path, unique_ref_to_ref_path, read_len):    """    For each sequence in the result find the best match in the DB    1. find the best match in the original bit format    2. find id's match in the dict file.    :return:    """    current_state = pd.read_csv(last_results_path, index_col=False)    current_state['Sequence'] = current_state.apply(lambda r: bin_to_sequence(r[CurrentStateFormat.Bases.A],                                                                              r[CurrentStateFormat.Bases.C],                                                                              r[CurrentStateFormat.Bases.G],                                                                              r[CurrentStateFormat.Bases.T],                                                                              2 * read_len), axis=1)    results_df = current_state[[CurrentStateFormat.Reference_id,                                CurrentStateFormat.Region,                                CurrentStateFormat.Priors,                                'Sequence']]    results_df['changed_reference_id'] = results_df[CurrentStateFormat.Reference_id]    results_df[CurrentStateFormat.Reference_id] = results_df[CurrentStateFormat.Reference_id].apply(lambda i: int(i))    logging.info("try to read csv: {}".format(unique_ref_to_ref_path))    if os.path.exists(unique_ref_to_ref_path):        logging.info("try to read csv: {}".format(unique_ref_to_ref_path))    else:        logging.info("try to read csv: {} - file not exists".format(unique_ref_to_ref_path))    ids_dict_df = pd.read_csv(unique_ref_to_ref_path, index_col=False)    full_result = results_df.merge(ids_dict_df, on=CurrentStateFormat.Reference_id)    logging.info("Headers: ids_dict_df = {}, full_result={}".format(ids_dict_df.columns, full_result.columns))    full_result = full_result[[CurrentStateFormat.Reference_id,                               ReferenceFormat.Original_Id,                               'changed_reference_id',                               CurrentStateFormat.Region,                               CurrentStateFormat.Priors,                               'Sequence']]    full_result.to_csv(final_results_path, index=False)def get_row_size(df):    if df is not None:        all_df_size = df.memory_usage(index=True, deep=True).sum()        return all_df_size / len(df)    return 0def get_maximal_superset(subsets_dict, curr_id, curr_subset):    curr_superset_len = len(curr_subset)    ret_value = curr_id    for k, v in subsets_dict.items():        if (v > curr_subset) and (len(v) > curr_superset_len):            ret_value = k            curr_superset_len = len(v)    return ret_valuedef main():    define_logger(logging.INFO)    iteration = get_emirge_iteration_mock()    iteration.do_iteration()if __name__ == '__main__':    main()